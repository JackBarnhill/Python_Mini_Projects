{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define time period\n",
    "years = np.arange(2025, 2028.1, 0.1)\n",
    "\n",
    "# Simulated models: Baseline economic impact of 3 Fed policy paths\n",
    "# GDP growth (%, annualized), Inflation (%), S&P 500 Index (arbitrary baseline 100), AE–OC \"Health Score\" (scale 0–100)\n",
    "\n",
    "def simulate_policy(rate_policy):\n",
    "    if rate_policy == \"cut\":\n",
    "        gdp = 2 + 0.5*np.sin(0.8*years)  # short-term GDP boost\n",
    "        inflation = 3 + 0.5*np.sin(1.2*years + 1) + 0.4*(years - 2025)  # inflation rises over time\n",
    "        sp500 = 100 + 10*(years - 2025)**1.5  # speculative growth\n",
    "        health = 80 - 15*(years - 2025)**0.9  # AE–OC score deteriorates\n",
    "    elif rate_policy == \"hold\":\n",
    "        gdp = 1.8 + 0.3*np.sin(0.7*years)\n",
    "        inflation = 2.2 + 0.2*np.sin(1.1*years + 1)\n",
    "        sp500 = 100 + 5*(years - 2025)**1.2\n",
    "        health = 80 + 2*(years - 2025)\n",
    "    elif rate_policy == \"raise\":\n",
    "        gdp = 1.5 + 0.2*np.sin(0.9*years) - 0.1*(years - 2025)\n",
    "        inflation = 1.8 + 0.1*np.sin(1.3*years + 2) - 0.1*(years - 2025)\n",
    "        sp500 = 100 + 3*(years - 2025)**1.1\n",
    "        health = 80 + 5*(years - 2025)**0.9\n",
    "    return gdp, inflation, sp500, health\n",
    "\n",
    "policies = [\"cut\", \"hold\", \"raise\"]\n",
    "results = {policy: simulate_policy(policy) for policy in policies}\n",
    "\n",
    "# Plot results\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "titles = [\"GDP Growth (%)\", \"Inflation (%)\", \"S&P 500 Index\", \"AE–OC Health Score\"]\n",
    "ylims = [(1, 4), (1, 5), (100, 200), (60, 100)]\n",
    "\n",
    "for i, metric in enumerate([\"GDP\", \"Inflation\", \"S&P 500\", \"AE–OC Health\"]):\n",
    "    ax = axs[i//2, i%2]\n",
    "    for policy in policies:\n",
    "        ax.plot(years, results[policy][i], label=policy.capitalize())\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_ylim(ylims[i])\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(titles[i])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f542c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE–OC Health Score Calculation Algorithm\n",
    "# This builds a full algorithmic structure for score calculation based on input values.\n",
    "\n",
    "def ae_oc_health_score(\n",
    "    fed_rate, core_inflation, pe_ratio,\n",
    "    private_investment_ratio, debt_to_gdp,\n",
    "    moral_hazard_score, gini_index,\n",
    "    trust_score, inflation_volatility\n",
    "):\n",
    "    # 1. Epistemological Clarity (30%)\n",
    "    # Interest Rate Gap Score\n",
    "    ideal_gap = 1.5\n",
    "    actual_gap = fed_rate - core_inflation\n",
    "    score_gap = max(0, 100 - 20 * abs(actual_gap - ideal_gap))\n",
    "\n",
    "    # P/E Ratio Score\n",
    "    ideal_pe = 15\n",
    "    score_pe = max(0, 100 - 5 * abs(pe_ratio - ideal_pe))\n",
    "\n",
    "    epistemic_score = (score_gap + score_pe) / 2\n",
    "\n",
    "    # 2. Causal-Systemic Integrity (25%)\n",
    "    # Private Investment / GDP Score\n",
    "    ideal_investment_ratio = 0.20\n",
    "    score_investment = min(100, (private_investment_ratio / ideal_investment_ratio) * 100)\n",
    "\n",
    "    # Debt-to-GDP Score\n",
    "    ideal_debt = 90\n",
    "    score_debt = max(0, 100 - 0.5 * abs(debt_to_gdp - ideal_debt))\n",
    "\n",
    "    systemic_score = (score_investment + score_debt) / 2\n",
    "\n",
    "    # 3. Ethical Consistency (20%)\n",
    "    # Moral Hazard Index (0 = perfect, 100 = total corruption)\n",
    "    score_moral_hazard = 100 - moral_hazard_score\n",
    "\n",
    "    # Gini Index Score\n",
    "    ideal_gini = 0.35\n",
    "    score_gini = max(0, 100 - 400 * abs(gini_index - ideal_gini))\n",
    "\n",
    "    ethical_score = (score_moral_hazard + score_gini) / 2\n",
    "\n",
    "    # 4. Institutional Trust (25%)\n",
    "    score_trust = trust_score  # 0–100 scale directly from surveys\n",
    "    ideal_volatility = 1.0\n",
    "    score_volatility = max(0, 100 - 25 * (inflation_volatility - ideal_volatility) ** 2)\n",
    "\n",
    "    trust_score_total = (score_trust + score_volatility) / 2\n",
    "\n",
    "    # Final AE–OC Health Score (Weighted Average)\n",
    "    total_score = (\n",
    "        0.30 * epistemic_score +\n",
    "        0.25 * systemic_score +\n",
    "        0.20 * ethical_score +\n",
    "        0.25 * trust_score_total\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Epistemological Clarity\": epistemic_score,\n",
    "        \"Systemic Integrity\": systemic_score,\n",
    "        \"Ethical Consistency\": ethical_score,\n",
    "        \"Institutional Trust\": trust_score_total,\n",
    "        \"AE–OC Health Score\": total_score\n",
    "    }\n",
    "\n",
    "ae_oc_health_score(\n",
    "    fed_rate=4.33,\n",
    "    core_inflation=1,\n",
    "    pe_ratio=24.20,\n",
    "    private_investment_ratio=0.18,\n",
    "    debt_to_gdp=130,\n",
    "    moral_hazard_score=50,\n",
    "    gini_index=0.39,\n",
    "    trust_score=55,\n",
    "    inflation_volatility=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d533dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = [\n",
    "    {\"zip\": \"77433\", \"neighborhood\": \"Bridgeland\", \"home_price\": 447995, \"monthly_rent\": 3291},\n",
    "    {\"zip\": \"77354\", \"neighborhood\": \"Magnolia\", \"home_price\": 331000, \"monthly_rent\": 1674},\n",
    "    {\"zip\": \"77429\", \"neighborhood\": \"Cypress\", \"home_price\": 390000, \"monthly_rent\": 1382},\n",
    "    {\"zip\": \"77377\", \"neighborhood\": \"Tomball\", \"home_price\": 355000, \"monthly_rent\": 1380},\n",
    "    {\"zip\": \"77493\", \"neighborhood\": \"North Katy\", \"home_price\": 334999, \"monthly_rent\": 2450},\n",
    "    {\"zip\": \"77447\", \"neighborhood\": \"Hockley\", \"home_price\": 328125, \"monthly_rent\": 2093},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def calculate_buy_score(row, ae_oc_score=71.635, moral_hazard=50):\n",
    "    rent_yield = (row['monthly_rent'] * 12) / row['home_price'] * 100\n",
    "    score = (\n",
    "        0.4 * ae_oc_score +\n",
    "        0.3 * rent_yield +\n",
    "        0.3 * (100 - moral_hazard)\n",
    "    )\n",
    "    return round(score, 2)\n",
    "\n",
    "df['buy_score'] = df.apply(calculate_buy_score, axis=1)\n",
    "\n",
    "df_sorted = df.sort_values(by='buy_score', ascending=False)\n",
    "print(df_sorted[['neighborhood', 'zip', 'home_price', 'monthly_rent', 'buy_score']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfcb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sector Score Calculator\n",
    "# B [Weight - 30%] = Bailout History (ratio of bailouts in 20th & 21st centuries * 100)\n",
    "# P [Weight - 25%] = Policy Dependency (estimated reliance on interest rates, subsidies, federal protection [Monetary Policy])\n",
    "# E [Weight - 25%] = Public Expectation (Is it 'Too big to fail'?)\n",
    "# A [Weight - 20%] = Moral Accountability (Can it fail without contagion fear or government rescue?)\n",
    "sector_data = [\n",
    "    {\"sector\": \"Banking\", \"B\": 95, \"P\": 90, \"E\": 90, \"A\": 30},\n",
    "    {\"sector\": \"Housing\", \"B\": 85, \"P\": 85, \"E\": 80, \"A\": 40},\n",
    "    {\"sector\": \"Higher Education\", \"B\": 70, \"P\": 75, \"E\": 80, \"A\": 30},\n",
    "    {\"sector\": \"Energy\", \"B\": 40, \"P\": 60, \"E\": 50, \"A\": 60},\n",
    "    {\"sector\": \"Technology\", \"B\": 30, \"P\": 40, \"E\": 50, \"A\": 70},\n",
    "    {\"sector\": \"Defense\", \"B\": 90, \"P\": 95, \"E\": 85, \"A\": 20},\n",
    "    {\"sector\": \"Crypto\", \"B\": 10, \"P\": 10, \"E\": 15, \"A\": 90},\n",
    "]\n",
    "\n",
    "moral_df = pd.DataFrame(sector_data)\n",
    "moral_df['MHS'] = (\n",
    "    0.3 * moral_df['B'] +\n",
    "    0.25 * moral_df['P'] +\n",
    "    0.25 * moral_df['E'] +\n",
    "    0.2 * (100 - moral_df['A'])\n",
    ")\n",
    "\n",
    "# Dashboard\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(moral_df['sector'], moral_df['MHS'], color='firebrick')\n",
    "plt.title(\"AE-OC Moral Hazard Score by Sector (2025 Estimate)\")\n",
    "plt.ylabel(\"Moral Hazard Score (0-100)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Portfolio Screener\n",
    "# Example: ETF/Stock - Exposure to Sectors\n",
    "portfolio = [\n",
    "    {\"ticker\": \"XLF\", \"name\": \"Financial Select SPDR\", \"exposure\": {\"Banking\": 0.9, \"Technology\": 0.1}},\n",
    "    {\"ticker\": \"XLRE\", \"name\": \"Real Estate Select SPDR\", \"exposure\": {\"Housing\": 0.8, \"Banking\": 0.2}},\n",
    "    {\"ticker\": \"ARKK\", \"name\": \"ARK Innovation ETF\", \"exposure\": {\"Technology\": 0.7, \"Crypto\": 0.3}},\n",
    "    {\"ticker\": \"XLE\", \"name\": \"Energy Select SPDR\", \"exposure\": {\"Energy\": 1.0}},\n",
    "    {\"ticker\": \"ITA\", \"name\": \"Aerospace & Defense ETF\", \"exposure\": {\"Defense\": 1.0}},\n",
    "]\n",
    "\n",
    "# Calculate Weighted MHS per ETF\n",
    "\n",
    "portfolio_scores = []\n",
    "for fund in portfolio:\n",
    "    score = 0\n",
    "    for sector, weight in fund['exposure'].items():\n",
    "        sector_score = moral_df.loc[moral_df['sector'] == sector, 'MHS'].values[0]\n",
    "        score += sector_score * weight\n",
    "    portfolio_scores.append({\"ticker\": fund['ticker'], \"name\": fund['name'], \"moral_hazard_score\": round(score, 2)})\n",
    "\n",
    "screener_df = pd.DataFrame(portfolio_scores).sort_values(by='moral_hazard_score', ascending=False)\n",
    "print(screener_df)\n",
    "\n",
    "# Highlight Safest vs. Riskiest\n",
    "safest = screener_df.loc[screener_df['moral_hazard_score'] < 50]\n",
    "riskiest = screener_df.loc[screener_df['moral_hazard_score'] > 70]\n",
    "print(\"\\nSafest Funds:\")\n",
    "print(safest)\n",
    "print(\"\\nRiskiest Funds:\")\n",
    "print(riskiest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152617a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This code was made by me after I discovered the Bresenham's Line Algorithm for approximating pixel coordinates of the arctangent function.\n",
    "#I thought it was really interesting, so I tried to see how it worked visually.\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def arctan_scaled(x, y_scale=201.6):\n",
    "    \"\"\"Scale arctan(x) to pixel space.\"\"\"\n",
    "    return int(round(math.atan(x) * y_scale))\n",
    "\n",
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    \"\"\"Standard Bresenham's line algorithm.\"\"\"\n",
    "    points = []\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    sx = 1 if x0 < x1 else -1\n",
    "    sy = 1 if y0 < y1 else -1\n",
    "    err = dx - dy\n",
    "\n",
    "    while True:\n",
    "        points.append((x0, y0))\n",
    "        if x0 == x1 and y0 == y1:\n",
    "            break\n",
    "        e2 = 2 * err\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x0 += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y0 += sy\n",
    "    return points\n",
    "\n",
    "def generate_arctan_curve(x_max=112, y_scale=201.6):\n",
    "    \"\"\"Generate pixel coordinates for the arctan curve.\"\"\"\n",
    "    curve_points = []\n",
    "    prev_x = 0\n",
    "    prev_y = arctan_scaled(prev_x, y_scale)\n",
    "\n",
    "    for x in range(1, x_max + 1):\n",
    "        y = arctan_scaled(x, y_scale)\n",
    "        # Apply Bresenham between each consecutive point\n",
    "        segment = bresenham_line(prev_x, prev_y, x, y)\n",
    "        curve_points.extend(segment)\n",
    "        prev_x = x\n",
    "        prev_y = y\n",
    "\n",
    "    return curve_points\n",
    "\n",
    "def plot_pixels(points, canvas_size=(350, 350)):\n",
    "    \"\"\"Plot pixels on a canvas.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(0, canvas_size[0])\n",
    "    ax.set_ylim(0, canvas_size[1])\n",
    "\n",
    "    # Invert Y-axis to mimic screen coordinate system\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Extract X and Y coordinates\n",
    "    x_vals = [p[0] for p in points]\n",
    "    y_vals = [p[1] for p in points]\n",
    "\n",
    "    # Plot as scatter points for a pixelated look\n",
    "    ax.scatter(x_vals, y_vals, c='black', marker='s', s=10)\n",
    "\n",
    "    ax.set_title(\"arctan(x) Curve (Bresenham Approximation)\")\n",
    "    ax.set_xlabel(\"x (pixels)\")\n",
    "    ax.set_ylabel(\"y (pixels)\")\n",
    "\n",
    "    # Enable zoom and pan from toolbar\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------- Main Execution ---------\n",
    "if __name__ == \"__main__\":\n",
    "    points = generate_arctan_curve()\n",
    "    plot_pixels(points)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
